#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸»æ§åˆ¶è„šæœ¬
åè°ƒå¤šçº¿ç¨‹å¤„ç†æµç¨‹ï¼ŒåŠ é€ŸAnalysisç›®å½•å®Œå–„
"""

import asyncio
import logging
from pathlib import Path
from datetime import datetime
import json
from typing import Dict, List

# å¯¼å…¥å·¥å…·æ¨¡å—
from automated_enhancement import AutomatedDocumentEnhancer
from document_cleanup import DocumentCleanupTool
from document_refactor import DocumentRefactorTool

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('analysis_enhancement.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class AnalysisEnhancementController:
    """Analysisç›®å½•å¢å¼ºæ§åˆ¶å™¨"""
    
    def __init__(self, base_dir: str = "docs/Analysis", max_workers: int = 8):
        self.base_dir = Path(base_dir)
        self.max_workers = max_workers
        self.start_time = datetime.now()
        
        # åˆå§‹åŒ–å·¥å…·
        self.enhancer = AutomatedDocumentEnhancer(base_dir, max_workers)
        self.cleanup_tool = DocumentCleanupTool(base_dir, max_workers)
        self.refactor_tool = DocumentRefactorTool(base_dir, max_workers)
        
        # è¿›åº¦è·Ÿè¸ª
        self.progress = {
            'phase': 'initialized',
            'completed_tasks': 0,
            'total_tasks': 0,
            'current_task': '',
            'errors': [],
            'warnings': []
        }
    
    async def run_full_enhancement_pipeline(self) -> Dict:
        """è¿è¡Œå®Œæ•´çš„å¢å¼ºæµæ°´çº¿"""
        logger.info("å¼€å§‹è¿è¡Œå®Œæ•´çš„Analysisç›®å½•å¢å¼ºæµæ°´çº¿")
        
        try:
            # é˜¶æ®µ1: æ–‡æ¡£æ¸…ç†
            await self._run_cleanup_phase()
            
            # é˜¶æ®µ2: æ–‡æ¡£é‡æ„
            await self._run_refactor_phase()
            
            # é˜¶æ®µ3: æ–‡æ¡£å¢å¼º
            await self._run_enhancement_phase()
            
            # é˜¶æ®µ4: è´¨é‡è¯„ä¼°
            await self._run_quality_assessment_phase()
            
            # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            final_report = await self._generate_final_report()
            
            logger.info("Analysisç›®å½•å¢å¼ºæµæ°´çº¿å®Œæˆ")
            return final_report
            
        except Exception as e:
            logger.error(f"å¢å¼ºæµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
            self.progress['errors'].append(str(e))
            raise
    
    async def _run_cleanup_phase(self):
        """è¿è¡Œæ¸…ç†é˜¶æ®µ"""
        logger.info("=== é˜¶æ®µ1: æ–‡æ¡£æ¸…ç† ===")
        self.progress['phase'] = 'cleanup'
        self.progress['current_task'] = 'æ–‡æ¡£æ¸…ç†'
        
        try:
            # ç”Ÿæˆæ¸…ç†è®¡åˆ’
            cleanup_plan = await self.cleanup_tool.generate_cleanup_plan()
            logger.info(f"æ¸…ç†è®¡åˆ’: åˆ é™¤{cleanup_plan['summary']['files_to_delete']}ä¸ªæ–‡ä»¶ï¼Œé‡å‘½å{cleanup_plan['summary']['files_to_rename']}ä¸ªæ–‡ä»¶")
            
            # æ‰§è¡Œæ¸…ç†
            cleanup_results = await self.cleanup_tool.run_full_cleanup()
            logger.info(f"æ¸…ç†å®Œæˆ: æˆåŠŸ{cleanup_results['summary']['total_successful']}ä¸ªï¼Œå¤±è´¥{cleanup_results['summary']['total_failed']}ä¸ª")
            
            # ä¿å­˜æ¸…ç†ç»“æœ
            cleanup_file = self.base_dir / "cleanup_results.json"
            with open(cleanup_file, 'w', encoding='utf-8') as f:
                json.dump(cleanup_results, f, ensure_ascii=False, indent=2)
            
            self.progress['completed_tasks'] += 1
            
        except Exception as e:
            logger.error(f"æ¸…ç†é˜¶æ®µå¤±è´¥: {e}")
            self.progress['errors'].append(f"æ¸…ç†é˜¶æ®µ: {e}")
            raise
    
    async def _run_refactor_phase(self):
        """è¿è¡Œé‡æ„é˜¶æ®µ"""
        logger.info("=== é˜¶æ®µ2: æ–‡æ¡£é‡æ„ ===")
        self.progress['phase'] = 'refactor'
        self.progress['current_task'] = 'æ–‡æ¡£é‡æ„'
        
        try:
            # å®šä¹‰é‡æ„ä»»åŠ¡
            refactor_tasks = [
                {
                    'file_path': self.base_dir / "01_Theoretical_Foundations/02_Cryptographic_Foundations/elliptic_curve_cryptography_web3.md",
                    'refactor_type': "elliptic_curve",
                    'priority': 1,
                    'target_quality': 90
                },
                {
                    'file_path': self.base_dir / "02_Core_Technologies/01_Blockchain_Fundamentals/blockchain_consensus_mechanisms_web3.md",
                    'refactor_type': "consensus_mechanism",
                    'priority': 1,
                    'target_quality': 90
                },
                {
                    'file_path': self.base_dir / "11_Advanced_Technologies/01_AI_Integration/ai_blockchain_integration_web3.md",
                    'refactor_type': "ai_integration",
                    'priority': 2,
                    'target_quality': 85
                },
                {
                    'file_path': self.base_dir / "12_Security_And_Verification/Security_And_Privacy/zero_knowledge_proof_theory_web3.md",
                    'refactor_type': "zero_knowledge",
                    'priority': 1,
                    'target_quality': 90
                }
            ]
            
            # æ‰§è¡Œé‡æ„
            for task in refactor_tasks:
                if task['file_path'].exists():
                    logger.info(f"é‡æ„æ–‡æ¡£: {task['file_path']}")
                    success = await self.refactor_tool.refactor_document(
                        task['file_path'], 
                        task['refactor_type']
                    )
                    if success:
                        logger.info(f"é‡æ„æˆåŠŸ: {task['file_path']}")
                    else:
                        logger.warning(f"é‡æ„å¤±è´¥: {task['file_path']}")
                        self.progress['warnings'].append(f"é‡æ„å¤±è´¥: {task['file_path']}")
                else:
                    logger.warning(f"æ–‡æ¡£ä¸å­˜åœ¨: {task['file_path']}")
                    self.progress['warnings'].append(f"æ–‡æ¡£ä¸å­˜åœ¨: {task['file_path']}")
            
            self.progress['completed_tasks'] += 1
            
        except Exception as e:
            logger.error(f"é‡æ„é˜¶æ®µå¤±è´¥: {e}")
            self.progress['errors'].append(f"é‡æ„é˜¶æ®µ: {e}")
            raise
    
    async def _run_enhancement_phase(self):
        """è¿è¡Œå¢å¼ºé˜¶æ®µ"""
        logger.info("=== é˜¶æ®µ3: æ–‡æ¡£å¢å¼º ===")
        self.progress['phase'] = 'enhancement'
        self.progress['current_task'] = 'æ–‡æ¡£å¢å¼º'
        
        try:
            # è¿è¡Œæ–‡æ¡£å¢å¼º
            enhancement_results = await self.enhancer.run_full_enhancement()
            logger.info(f"å¢å¼ºå®Œæˆ: æ€»æ–‡æ¡£{enhancement_results['total_documents']}ä¸ªï¼Œéœ€è¦å¢å¼º{enhancement_results['needs_enhancement']}ä¸ª")
            
            # ä¿å­˜å¢å¼ºç»“æœ
            enhancement_file = self.base_dir / "enhancement_results.json"
            with open(enhancement_file, 'w', encoding='utf-8') as f:
                json.dump(enhancement_results, f, ensure_ascii=False, indent=2)
            
            self.progress['completed_tasks'] += 1
            
        except Exception as e:
            logger.error(f"å¢å¼ºé˜¶æ®µå¤±è´¥: {e}")
            self.progress['errors'].append(f"å¢å¼ºé˜¶æ®µ: {e}")
            raise
    
    async def _run_quality_assessment_phase(self):
        """è¿è¡Œè´¨é‡è¯„ä¼°é˜¶æ®µ"""
        logger.info("=== é˜¶æ®µ4: è´¨é‡è¯„ä¼° ===")
        self.progress['phase'] = 'quality_assessment'
        self.progress['current_task'] = 'è´¨é‡è¯„ä¼°'
        
        try:
            # æ‰«ææ‰€æœ‰æ–‡æ¡£è¿›è¡Œè´¨é‡è¯„ä¼°
            all_files = list(self.base_dir.rglob('*.md'))
            quality_results = []
            
            for file_path in all_files:
                quality_result = await self.enhancer.analyze_document_quality(file_path)
                quality_results.append(quality_result)
            
            # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
            quality_report = {
                'timestamp': datetime.now().isoformat(),
                'total_documents': len(all_files),
                'quality_distribution': {
                    'excellent': len([r for r in quality_results if r['quality_score'] >= 90]),
                    'good': len([r for r in quality_results if 80 <= r['quality_score'] < 90]),
                    'fair': len([r for r in quality_results if 70 <= r['quality_score'] < 80]),
                    'poor': len([r for r in quality_results if r['quality_score'] < 70])
                },
                'average_score': sum(r['quality_score'] for r in quality_results) / len(quality_results),
                'detailed_results': quality_results
            }
            
            # ä¿å­˜è´¨é‡æŠ¥å‘Š
            quality_file = self.base_dir / "quality_assessment.json"
            with open(quality_file, 'w', encoding='utf-8') as f:
                json.dump(quality_report, f, ensure_ascii=False, indent=2)
            
            logger.info(f"è´¨é‡è¯„ä¼°å®Œæˆ: å¹³å‡åˆ†æ•°{quality_report['average_score']:.2f}")
            self.progress['completed_tasks'] += 1
            
        except Exception as e:
            logger.error(f"è´¨é‡è¯„ä¼°é˜¶æ®µå¤±è´¥: {e}")
            self.progress['errors'].append(f"è´¨é‡è¯„ä¼°é˜¶æ®µ: {e}")
            raise
    
    async def _generate_final_report(self) -> Dict:
        """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
        end_time = datetime.now()
        duration = end_time - self.start_time
        
        final_report = {
            'project': 'Analysisç›®å½•å¢å¼ºé¡¹ç›®',
            'start_time': self.start_time.isoformat(),
            'end_time': end_time.isoformat(),
            'duration_seconds': duration.total_seconds(),
            'progress': self.progress,
            'summary': {
                'total_phases': 4,
                'completed_phases': self.progress['completed_tasks'],
                'success_rate': (self.progress['completed_tasks'] / 4) * 100,
                'total_errors': len(self.progress['errors']),
                'total_warnings': len(self.progress['warnings'])
            },
            'recommendations': self._generate_recommendations()
        }
        
        # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
        final_report_file = self.base_dir / "final_enhancement_report.json"
        with open(final_report_file, 'w', encoding='utf-8') as f:
            json.dump(final_report, f, ensure_ascii=False, indent=2)
        
        return final_report
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if self.progress['errors']:
            recommendations.append("éœ€è¦è§£å†³æ‰€æœ‰é”™è¯¯ä»¥ç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§")
        
        if self.progress['warnings']:
            recommendations.append("å»ºè®®å¤„ç†è­¦å‘Šä»¥æé«˜æ–‡æ¡£è´¨é‡")
        
        if self.progress['completed_tasks'] < 4:
            recommendations.append("éƒ¨åˆ†é˜¶æ®µæœªå®Œæˆï¼Œå»ºè®®é‡æ–°è¿è¡Œ")
        
        recommendations.extend([
            "å®šæœŸè¿è¡Œè´¨é‡è¯„ä¼°ä»¥ç›‘æ§æ–‡æ¡£è´¨é‡",
            "å»ºç«‹è‡ªåŠ¨åŒ–æµ‹è¯•æµç¨‹",
            "å®æ–½æŒç»­é›†æˆæœºåˆ¶",
            "å»ºç«‹æ–‡æ¡£ç»´æŠ¤æ ‡å‡†"
        ])
        
        return recommendations
    
    async def run_parallel_enhancement(self):
        """å¹¶è¡Œè¿è¡Œå¢å¼ºä»»åŠ¡"""
        logger.info("å¼€å§‹å¹¶è¡Œè¿è¡Œå¢å¼ºä»»åŠ¡")
        
        try:
            # å¹¶è¡Œæ‰§è¡Œæ¸…ç†å’Œé‡æ„
            cleanup_task = asyncio.create_task(self._run_cleanup_phase())
            refactor_task = asyncio.create_task(self._run_refactor_phase())
            
            # ç­‰å¾…ç¬¬ä¸€é˜¶æ®µå®Œæˆ
            await asyncio.gather(cleanup_task, refactor_task)
            
            # å¹¶è¡Œæ‰§è¡Œå¢å¼ºå’Œè´¨é‡è¯„ä¼°
            enhancement_task = asyncio.create_task(self._run_enhancement_phase())
            quality_task = asyncio.create_task(self._run_quality_assessment_phase())
            
            # ç­‰å¾…ç¬¬äºŒé˜¶æ®µå®Œæˆ
            await asyncio.gather(enhancement_task, quality_task)
            
            # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            final_report = await self._generate_final_report()
            
            logger.info("å¹¶è¡Œå¢å¼ºä»»åŠ¡å®Œæˆ")
            return final_report
            
        except Exception as e:
            logger.error(f"å¹¶è¡Œå¢å¼ºä»»åŠ¡å¤±è´¥: {e}")
            raise

async def main():
    """ä¸»å‡½æ•°"""
    logger.info("å¯åŠ¨Analysisç›®å½•å¢å¼ºæ§åˆ¶å™¨")
    
    # åˆ›å»ºæ§åˆ¶å™¨å®ä¾‹
    controller = AnalysisEnhancementController(max_workers=8)
    
    try:
        # è¿è¡Œå®Œæ•´æµæ°´çº¿
        final_report = await controller.run_full_enhancement_pipeline()
        
        # è¾“å‡ºç»“æœ
        print("\n" + "="*60)
        print("ğŸ‰ Analysisç›®å½•å¢å¼ºé¡¹ç›®å®Œæˆï¼")
        print("="*60)
        print(f"é¡¹ç›®å¼€å§‹æ—¶é—´: {final_report['start_time']}")
        print(f"é¡¹ç›®ç»“æŸæ—¶é—´: {final_report['end_time']}")
        print(f"æ€»è€—æ—¶: {final_report['duration_seconds']:.2f} ç§’")
        print(f"å®Œæˆé˜¶æ®µ: {final_report['summary']['completed_phases']}/{final_report['summary']['total_phases']}")
        print(f"æˆåŠŸç‡: {final_report['summary']['success_rate']:.1f}%")
        print(f"é”™è¯¯æ•°é‡: {final_report['summary']['total_errors']}")
        print(f"è­¦å‘Šæ•°é‡: {final_report['summary']['total_warnings']}")
        
        print("\nğŸ“‹ æ”¹è¿›å»ºè®®:")
        for i, rec in enumerate(final_report['recommendations'], 1):
            print(f"  {i}. {rec}")
        
        print(f"\nğŸ“Š è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {controller.base_dir}/final_enhancement_report.json")
        
    except Exception as e:
        logger.error(f"ä¸»ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥æ—¥å¿—æ–‡ä»¶è·å–è¯¦ç»†ä¿¡æ¯")

if __name__ == "__main__":
    asyncio.run(main())
